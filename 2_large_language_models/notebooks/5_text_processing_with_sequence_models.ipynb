{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generaing Shakespeare Text with a Character-based RNN\n",
    "\n",
    "Now we'll take a look at using RNNs to generate text that's similar to Shakespeare's content.\n",
    "\n",
    "We download the shakespeare data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "print(len(shakespeare_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to encode this text as vectors, using a character-based encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 12:56:57.384880: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare characters:\n",
      "First Citi\n",
      "Encoded text:\n",
      "[21  7 10  9  4  2 20  7  4  7]\n",
      "Note the 'i's, represented by 7's in the 2nd, 8th and 10th positions\n"
     ]
    }
   ],
   "source": [
    "# split based on characters\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "# fit the text vectorizer on our text\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]\n",
    "print(f\"Shakespeare characters:\\n{shakespeare_text[:10]}\")\n",
    "print(f\"Encoded text:\\n{encoded[:10]}\")\n",
    "print(f\"Note the 'i's, represented by 7's in the 2nd, 8th and 10th positions\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need a function that will generate a `tf.data.Dataset` from the input sequence of integers. \n",
    "\n",
    "We need to:\n",
    "\n",
    "* have a sliding window\n",
    "* batch the windows into batches\n",
    "* shuffle those samples (note we can do this here, even though it's a time series forecasting task, because our window includes our label; so our data is alread in X-y form)\n",
    "* generate tuples of (X, y) pairs from our shuffled batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32, prefetch=True):\n",
    "    \"\"\"\n",
    "    Given a sequence of tensors, returns a dataset of windows of that sequence.\n",
    "    \"\"\"\n",
    "    # create a tf.data.Dataset from a Tensor sequence\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    # create a window iterator over the tensors - note that this is length + 1, so it'll include our label\n",
    "    dataset = dataset.window(length + 1, shift=1, drop_remainder=True)\n",
    "    # batch each sample \n",
    "    dataset = dataset.flat_map(lambda window: window.batch(length + 1))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=100_000, seed=seed)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    # split the samples into X and y, then activate prefetching\n",
    "    if prefetch:\n",
    "        return dataset.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
    "    else:\n",
    "        return dataset.map(lambda window: (window[:, :-1], window[:, 1:]))\n",
    "\n",
    "# set the window length\n",
    "length = 100 \n",
    "# set a random seed\n",
    "seed=42\n",
    "tf.random.set_seed(seed)\n",
    "# generate iterables from the encoded text\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=seed)\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can build a Character-based RNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 13:19:40.390480: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-06-30 13:19:46.499920: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-06-30 13:19:46.660644: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  31247/Unknown - 756s 24ms/step - loss: 1.4022 - accuracy: 0.5712"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 13:32:16.156344: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10294842070907974379\n",
      "2023-06-30 13:32:16.156356: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16869031335745393553\n",
      "2023-06-30 13:32:16.156360: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12493847632041027229\n",
      "2023-06-30 13:32:16.156364: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8800648251343980905\n",
      "2023-06-30 13:32:16.156390: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16716365723107324064\n",
      "2023-06-30 13:32:16.156394: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12314640515660100190\n",
      "2023-06-30 13:32:16.156399: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3701914617589568570\n",
      "2023-06-30 13:32:16.156403: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 183830168126011872\n",
      "2023-06-30 13:32:16.156408: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17502380470954015236\n",
      "2023-06-30 13:32:16.156413: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14164193168255445810\n",
      "2023-06-30 13:32:16.156416: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14859036211853249764\n",
      "2023-06-30 13:32:16.156419: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12172555526055049820\n",
      "2023-06-30 13:32:16.312247: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-06-30 13:32:16.381092: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-06-30 13:32:33.362670: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15295726430844723283\n",
      "2023-06-30 13:32:33.362684: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17078253016795014615\n",
      "2023-06-30 13:32:33.362687: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 7716381634434930477\n",
      "2023-06-30 13:32:33.362701: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4811186469637306927\n",
      "2023-06-30 13:32:33.362704: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 11062228684818762983\n",
      "2023-06-30 13:32:33.362708: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3821362366183124699\n",
      "2023-06-30 13:32:33.362713: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8366187092557836534\n",
      "2023-06-30 13:32:33.362716: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8929784647405871620\n",
      "2023-06-30 13:32:33.362719: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1872666708205506306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 775s 25ms/step - loss: 1.4022 - accuracy: 0.5712 - val_loss: 1.5974 - val_accuracy: 0.5341\n",
      "Epoch 2/2\n",
      "31247/31247 [==============================] - ETA: 0s - loss: 1.2993 - accuracy: 0.5953INFO:tensorflow:Assets written to: shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 762s 24ms/step - loss: 1.2993 - accuracy: 0.5953 - val_loss: 1.5806 - val_accuracy: 0.5416\n",
      "CPU times: user 23min 52s, sys: 8min 15s, total: 32min 7s\n",
      "Wall time: 25min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_tokens = text_vec_layer.vocabulary_size()\n",
    "path_to_saved_shakespeare_character_rnn = pathlib.Path(\"shakespeare_model\")\n",
    "\n",
    "if path_to_saved_shakespeare_character_rnn.exists():\n",
    "    model = keras.models.load_model(path_to_saved_shakespeare_character_rnn)\n",
    "else:\n",
    "    model = tf.keras.Sequential([\n",
    "        # encode integers to dense vectors\n",
    "        tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "        tf.keras.layers.GRU(128, return_sequences=True),\n",
    "        # we need n_tokens units - this has to match your vocab size!\n",
    "        tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=\"nadam\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"shakespeare_model\",\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_set,\n",
    "        validation_data=valid_set,\n",
    "        epochs=2,\n",
    "        callbacks=[model_ckpt]\n",
    "    )\n",
    "    model.save(path_to_saved_shakespeare_character_rnn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take that model, and wrap it into another model that will perform the character embedding step to make it easier for us to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    model\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can test our model at predicting the next **letter** of a sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "Input sentence: To be or not to b\n",
      "Predicted character: e\n"
     ]
    }
   ],
   "source": [
    "example_sentence = \"To be or not to b\"\n",
    "prediction_probabilities = shakespeare_model.predict([example_sentence])[0, -1]\n",
    "prediction = tf.argmax(prediction_probabilities)\n",
    "print(f\"Input sentence: {example_sentence}\")\n",
    "print(f\"Predicted character: {text_vec_layer.get_vocabulary()[prediction]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we've correctly predicted the final `e` on \"To be or not to be\".\n",
    "\n",
    "Now, let's trying generating some Shakespearean text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_character(text, temperature=1):\n",
    "    probabilities = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(probabilities) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id]\n",
    "\n",
    "def generate_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += predict_next_character(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 222ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 18:31:24.722212: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-06-30 18:31:24.820394: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "To be or not to be a strange,\n",
      "and the duke is a man and the state,\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "print(generate_text(\"To be or not to be\", temperature=0.01))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with Transformer\n",
    "\n",
    "In this section, we'll implement a custom Transformer block in Keras, and use it to classify text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can download the dataset (IMDB movie reviews):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 0s 0us/step\n",
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 17:57:09.549088: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - ETA: 0s - loss: 0.4141 - accuracy: 0.7955"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 17:57:50.591405: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 58s 71ms/step - loss: 0.4141 - accuracy: 0.7955 - val_loss: 0.3077 - val_accuracy: 0.8666\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.2148 - accuracy: 0.9162 - val_loss: 0.3296 - val_accuracy: 0.8728\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with Hugging Face and DistilBERT\n",
    "\n",
    "In this section, we'll use the popular Hugging Face libraries to download a trained version of DistilBERT, and perform text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look how easy it is to download a pre-trained model artifact, and run tasks using HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ddd2c29e974f76baab5f424a30fb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/729 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afad6c8d1e7b484b8c936c3ba2a64c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6929c6bbbf674110a19549611de8c50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/58.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26672557a7584a92b5a58ac7ef0c8e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe2236743ff437bad73ef3b35d2ba6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8393d9252dcf46069a359457ed098b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'contradiction', 'score': 0.984832227230072}]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "result = classifier(\"She loves me. She loves me not.\")\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we can see that the model correctly identified this sentence as a CONTRADICTION, with a confidence of 98.48%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-masterclass--jDPYBdG-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
