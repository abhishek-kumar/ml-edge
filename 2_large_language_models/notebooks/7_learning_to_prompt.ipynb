{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Libraries, API keys and imports.\n",
        "!pip install -qU langchain\n",
        "!pip install -qU huggingface_hub\n",
        "!pip install -qU openai\n",
        "!pip install -qU google-cloud-aiplatform \"shapely<2\"\n",
        "import os\n",
        "\n",
        "# Generate a new key here: https://platform.openai.com/account/api-keys\n",
        "os.environ['OPENAI_API_KEY'] = \"your_key_here\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "198ubssPmfJ6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gcloud setup to use VertexAI.\n",
        "!gcloud auth application-default login\n",
        "!gcloud config set project stellar-works-391900"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "j_PGZ6RejDET",
        "outputId": "3e429ed1-893c-42dc-8ac7-edea7a439c62"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=pZDP4j7MSStVuWC7JuBkFWvmYTnJYl&prompt=consent&access_type=offline&code_challenge=M1X37MlNa8woJ_hl5NUBzhv4379x2GBWpPggqejil9U&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AZEOvhVTF-Oj7xD3dbjqRMe7LHA203TA6LKCW0y4F0cW-WIGGwNXM6HQ94_LryW8LUDTwg\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\n",
            "Quota project \"stellar-works-391900\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n",
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Instruction following.\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.llms import VertexAI\n",
        "\n",
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0.9)\n",
        "# llm = VertexAI()\n",
        "text = \"Suggest a personalized workout routine for someone looking to improve heart health outdoors.\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "INeN5x9cmWws",
        "outputId": "f6a9ea2a-8df5-401c-cbd2-c38b1df39cbc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Start with a 10-minute warmup, such as walking, jogging, or cycling at a moderate pace.\n",
            "\n",
            "2. Do 20 minutes of interval training by increasing your pace to a fast run for 1 minute, then slowing down for 2 minutes. Repeat this cycle 8 times.\n",
            "\n",
            "3. Follow this with 10 minutes of total body stretching.\n",
            "\n",
            "4. Jog or walk briskly for 15 minutes, followed by a 5-minute cool down.\n",
            "\n",
            "5. On alternate days, try going for a 30-minute power walk or a 30-minute light jog.\n",
            "\n",
            "6. Add 30 minutes of swimming or cycling for 2 or 3 days a week.\n",
            "\n",
            "7. Finish with 10 minutes of stretching and 5 minutes of deep breathing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. LLMChain - prompt template with embedded inputs.\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"course\"],\n",
        "    template=\"What is a good name for a company that teaches {course}?\",\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "print(chain.run(\"recommendation systems\"))\n",
        "# TODO: build a pydantic output parser to programmatically extract the answer."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "yIaGqwEhna5p",
        "outputId": "9c00869c-b39a-4c4d-a800-4bc10dd6c920"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "RecSysTutor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. ConversationChain with memory.\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "# conversation\n",
        "conversation.predict(input=\"Introduce yourself.\")\n",
        "conversation.predict(input=\"What can you do?\")\n",
        "conversation.predict(input=\"How can you help me with teaching LLMs?\")\n",
        "\n",
        "print(conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "LmNMVHaBn3Ib",
        "outputId": "0592207f-ceb6-4610-82a7-c886228288c5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Introduce yourself.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Introduce yourself.\n",
            "AI:  Hi there! I'm a chatbot created to help make your day a little easier. My name is Botty and I'm here to help answer any questions or lend an ear if you need to chat. What can I do for you today?\n",
            "Human: What can you do?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Introduce yourself.\n",
            "AI:  Hi there! I'm a chatbot created to help make your day a little easier. My name is Botty and I'm here to help answer any questions or lend an ear if you need to chat. What can I do for you today?\n",
            "Human: What can you do?\n",
            "AI:  I'm designed to provide customer support by helping you find the answers to questions you have such as product information, policies, and troubleshooting. I also have some cool conversations functions that you can explore. What can I help you with today?\n",
            "Human: How can you help me with teaching LLMs?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Introduce yourself.', additional_kwargs={}, example=False), AIMessage(content=\" Hi there! I'm a chatbot created to help make your day a little easier. My name is Botty and I'm here to help answer any questions or lend an ear if you need to chat. What can I do for you today?\", additional_kwargs={}, example=False), HumanMessage(content='What can you do?', additional_kwargs={}, example=False), AIMessage(content=\" I'm designed to provide customer support by helping you find the answers to questions you have such as product information, policies, and troubleshooting. I also have some cool conversations functions that you can explore. What can I help you with today?\", additional_kwargs={}, example=False), HumanMessage(content='How can you help me with teaching LLMs?', additional_kwargs={}, example=False), AIMessage(content=\" I'm sorry, but I'm not set up to help with teaching LLMs. However, I can point you in the direction of some helpful resources that may be able to provide further assistance. Would you like me to do that for you?\", additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history') callbacks=None callback_manager=None verbose=True tags=None prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True) llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.9, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all', tiktoken_model_name=None) output_key='response' output_parser=NoOpOutputParser() return_final_only=True llm_kwargs={} input_key='input'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. LLMChain with prompt template to generate recommendations.\n",
        "\n",
        "# Clear instructions.\n",
        "# Specificity.\n",
        "# Open-ended creativity.\n",
        "# Focus on the task.\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"theme\", \"year\"],\n",
        "    template=\"You must help me come up with a song title. What's a good song title for a song about {theme} in the year {year}?\",\n",
        ")\n",
        "\n",
        "# Create LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "response = chain.run({\n",
        "    \"theme\": \"interstellar travel\",\n",
        "    \"year\": \"3030\"\n",
        "})\n",
        "\n",
        "print(\"Theme: interstellar travel\")\n",
        "print(\"Year: 3030\")\n",
        "print(\"AI-generated song title:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "4UJt_ZkQcp4X",
        "outputId": "48f1456f-ac29-407d-8c14-cf6aba3ad37e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theme: interstellar travel\n",
            "Year: 3030\n",
            "AI-generated song title: \n",
            "\n",
            "\"Interstellar Odyssey: 3030\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5. Few-shot prompting.\n",
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"animal\": \"lion\", \"habitat\": \"savanna\"},\n",
        "    {\"animal\": \"polar bear\", \"habitat\": \"Arctic ice\"},\n",
        "    {\"animal\": \"elephant\", \"habitat\": \"African grasslands\"}\n",
        "]\n",
        "\n",
        "example_template = \"\"\"\n",
        "Animal: {animal}\n",
        "Habitat: {habitat}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"animal\", \"habitat\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "dynamic_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Identify the habitat of the given animal\",\n",
        "    suffix=\"Animal: {input}\\nHabitat:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\\n\",\n",
        ")\n",
        "\n",
        "# Create the LLMChain for the dynamic_prompt\n",
        "chain = LLMChain(llm=llm, prompt=dynamic_prompt)\n",
        "\n",
        "# Run the LLMChain with input_data\n",
        "input_data = {\"input\": \"tiger\"}\n",
        "response = chain.run(input_data)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "EuBlDuWrcQTW",
        "outputId": "287716d1-abe0-405d-c3a8-8748c91d4e9b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " tropical and subtropical forests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6. Extracting knowledge triplets.\n",
        "# Source: https://github.com/hwchase17/langchain/blob/master/langchain/indexes/prompts/knowledge_triplet_extraction.py\n",
        "\n",
        "from langchain.graphs.networkx_graph import KG_TRIPLE_DELIMITER\n",
        "\n",
        "# Prompt template for knowledge triple extraction\n",
        "_DEFAULT_KNOWLEDGE_TRIPLE_EXTRACTION_TEMPLATE = (\n",
        "    \"You are a networked intelligence helping a human track knowledge triples\"\n",
        "    \" about all relevant people, things, concepts, etc. and integrating\"\n",
        "    \" them with your knowledge stored within your weights\"\n",
        "    \" as well as that stored in a knowledge graph.\"\n",
        "    \" Extract all of the knowledge triples from the text.\"\n",
        "    \" A knowledge triple is a clause that contains a subject, a predicate,\"\n",
        "    \" and an object. The subject is the entity being described,\"\n",
        "    \" the predicate is the property of the subject that is being\"\n",
        "    \" described, and the object is the value of the property.\\n\\n\"\n",
        "    \"EXAMPLE\\n\"\n",
        "    \"It's a state in the US. It's also the number 1 producer of gold in the US.\\n\\n\"\n",
        "    f\"Output: (Nevada, is a, state){KG_TRIPLE_DELIMITER}(Nevada, is in, US)\"\n",
        "    f\"{KG_TRIPLE_DELIMITER}(Nevada, is the number 1 producer of, gold)\\n\"\n",
        "    \"END OF EXAMPLE\\n\\n\"\n",
        "    \"EXAMPLE\\n\"\n",
        "    \"I'm going to the store.\\n\\n\"\n",
        "    \"Output: NONE\\n\"\n",
        "    \"END OF EXAMPLE\\n\\n\"\n",
        "    \"EXAMPLE\\n\"\n",
        "    \"Oh huh. I know Descartes likes to drive antique scooters and play the mandolin.\\n\"\n",
        "    f\"Output: (Descartes, likes to drive, antique scooters){KG_TRIPLE_DELIMITER}(Descartes, plays, mandolin)\\n\"\n",
        "    \"END OF EXAMPLE\\n\\n\"\n",
        "    \"EXAMPLE\\n\"\n",
        "    \"{text}\"\n",
        "    \"Output:\"\n",
        ")\n",
        "\n",
        "KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=_DEFAULT_KNOWLEDGE_TRIPLE_EXTRACTION_TEMPLATE,\n",
        ")\n",
        "\n",
        "# Create an LLMChain using the knowledge triple extraction prompt\n",
        "chain = LLMChain(llm=llm, prompt=KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT)\n",
        "\n",
        "# Run the chain with the specified text\n",
        "text = \"The city of Paris is the capital and most populous city of France. The Eiffel Tower is a famous landmark in Paris.\"\n",
        "triples = chain.run(text)\n",
        "\n",
        "print(triples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "-DTPeOkqgZb2",
        "outputId": "933dfb4b-7089-4ed0-b3c8-e397f19ec851"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " (Paris, is the capital of, France)<|>(Paris, is the most populous city of, France)<|>(Eiffel Tower, is a, landmark)<|>(Eiffel Tower, is in, Paris)\n"
          ]
        }
      ]
    }
  ]
}
